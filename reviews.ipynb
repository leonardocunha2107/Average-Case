{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTu2\n",
    "\n",
    "We thank you for bringing the typos to  our attention and we have corrected the numbering format. We also highlight that Algorithm 1 is a novel contribution in page 5 and introduce the abreviation in page 3.  \n",
    "Concerning the limitations of the paper, a proper analysis of optimization algorithms, on the average-case or worst-case sense, outside the quadratic setting is indeed a major open problem. Most results outside the setting are only asymptotical and boil down to the notion that any function is similar to a quadratic near it's optimum.\n",
    "Concerning your question about the concentration of the 'complexities' Paquette et al. (https://arxiv.org/pdf/2006.04299.pdf) show that under reasonable assumptions there's tight concentration around the mean in high dimensions. Our experiments also support this notion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VZRV\n",
    "\n",
    "We'd like to highlight that the Assumption 1 puts any smooth and non-strongly convex in one of the 'classes' $\\nu_{\\xi,\\tau}$, so our analysis cover any such distribution, i.e. the entire convex regime. In this sense Assumption 1 is not at all restrictive.   \n",
    "To get results valid over each of theses classes we need to leave the 'precise' (non-asymptotic) rates for the asymptotic ones, there's indeed a tradeoff between the granularity of the rates and the granularity of the classes of distribution we consider. Our main contribution is showing that an important aspect of the convergence, the asymptotic rate, is depedent only on the concentration around the edges and our experiment show that our claims are qualitatevely valid on the non-asymptotic regime.  \n",
    "The 'characteristic values' are indeed hard to estimate in practice, our contribution in that sense is showing which algorithms are faster over a range of what we consider practical values, i.e. $\\xi \\approx -1/2$\n",
    "We clarified the proof that the rates we give are optimal. The main point of the proof is that the optimal polynomial w.r.t. to a $\\nu$ following Assumption 1 must have the same rate on $\\nu$ as it does on the corresponding Beta distribution. Eqs. 50, 51 and 52 wotk similarly to the proof of theorem 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tq5M\n",
    "\n",
    "Your observation about the mass in $0$ is correct. We add a coment on this right after Assumption 1. Also an explanation about the blue lines on figures 5 and 7\n",
    "Our main contribution is showing that an important aspect of the convergence, the asymptotic rate, is depedent only on the concentration around the edges and our experiment show that our claims are qualitatevely valid on the non-asymptotic regime. Though the work draws heavily from (Paquette et al. 2020) and (Pedregosa et al. 2020) we believe the realization that the asymptotic rates are dependent only on the concentrations is a highly non-trivial one, that this adds to the understanding of the optimization dynamics at an intuitive level and why the usual algorithms show the performance they do in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## poH2\n",
    "\n",
    "Adressing the minor issues you pointed:  \n",
    "We mention the QuickSort algorithm as we believe this is the main setting where a non-expert reader might be familiar with average-case analysis.  \n",
    "'Objective $l$'  subsums the gradient norm, function-value and distance to the optimum metrics. We use this terminology to refer to the three of them, and their associated optimal methods in a concise fashion.  \n",
    "A recent work (https://arxiv.org/pdf/2005.01100.pdf) shows certain random matrix show a E.S.D. shaped as Beta distribution. This, however, is not important for our manuscript as the Beta distributions show up first as a generalization of the Marchenko Pastur distribution and second, from the proof p.o.v., as an element of the 'equivalence classes' $\\nu_{\\tau,\\xi}$ for which we can compute the precise rates.  \n",
    "We added a clarification on the Jacobi recurrence on Appendix B.\n",
    "\n",
    "Regarding the Major Issues you pointed:\n",
    "The optimal way, w.r.t. objective $l$, to choose $\\alpha$, $\\beta$ given $\\tau,\\xi$ is indeed given on the paragraph you mentioned. Could you clarify why you said ' However, it does not seem to be the case'.  \n",
    "There seems to be major misunderstanding here. Any distribution non-strongly convex smooth distribution fits into Assumption 1, as any distribution shows one value for the concentration near each of it's edges. The assumption effectively splits the 'convex regime' into equivalence classes, and our results cover any such distribution.\n",
    "Our main contribution is showing that an important aspect of the convergence, the asymptotic rate, is depedent only on the concentration around the edges and our experiment show that our claims are qualitatevely valid on the non-asymptotic regime. Though the work draws heavily from (Paquette et al. 2020) and (Pedregosa et al. 2020) we believe the realization that the asymptotic rates are dependent only on the concentrations is a highly non-trivial one, that this adds to the understanding of the optimization dynamics at an intuitive level and why the usual algorithms show the performance they do in practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
